{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\vansh\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "hf_api_token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "login(hf_api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\vansh\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#This library helps in callin the model for inference only, without downloading it\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "repo=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm = HuggingFaceEndpoint(repo_id=repo, max_length=128, temperature=0.7, token=hf_api_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The average distance from the sun to the earth is about 93 million miles. This distance is known as an astronomical unit (AU).\\n\\nThe Earth's orbit around the sun is not a perfect circle, but an ellipse with the sun at one focus. The Earth's closest approach to the sun, known as perihelion, occurs on January 3, and the Earth's furthest distance from the sun, known as aphelion, occurs on July 4. At perihelion, the Earth is about 91.4 million miles from the sun, and at aphelion it is about 94.5 million miles from the sun.\\n\\nThe distance between the Earth and the sun varies due to the elliptical shape of the Earth's orbit and the gravitational influences of other planets. The Earth's distance from the sun is also affected by the gravitational forces of the moon, which causes the Earth's orbit to slightly change over time. This is known as the Earth's orbital precession, and it takes about 26,000 years for the Earth to complete one full precession.\\n\\nThe distance between the Earth and the sun is important for understanding the Earth's climate, the Earth's rotation, and the Earth's orbit around the sun. The distance between the Earth and the sun affects the amount of solar radiation that reaches the Earth, which in turn affects the Earth's climate. The Earth's distance from the sun also affects the Earth's rotation, as the gravitational forces of the sun and the moon cause the Earth to wobble slightly as it orbits the sun. The Earth's orbit around the sun is also important for understanding the Earth's seasons and the cycles of day and night.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"How far is the earth from the sun?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] template='Question :  {question}\\nAnswer: What is his top speed? What is the record set by him?'\n"
     ]
    }
   ],
   "source": [
    "#let's create some prompt design for our model\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "question = \"Who is the fastest man on earth?\"\n",
    "template = \"\"\"Question :  {question}\n",
    "Answer: What is his top speed? What is the record set by him?\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variable=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vansh\\anaconda3\\envs\\new_genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who is the fastest man on earth?', 'text': '\\n\\nThe fastest man on earth is Usain Bolt. He has set the world record for the 100 meters sprint with a time of 9.58 seconds, which was set at the World Athletics Championships in Berlin, Germany, in 2009. His top speed in that race was 27.8 miles per hour (44.7 kilometers per hour).'}\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vansh\\anaconda3\\envs\\new_genai\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Now let's nowload a model locally instead of just using it for inference\"\"\"\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad643bef39114e519761069ae3f3e00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vansh\\anaconda3\\envs\\new_genai\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vansh\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48ce810143643abafd82fdb75e03cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8b68a4fd9b4c71b26d9c282a823bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c139673c13543e1ac54bacdbf100e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_Id = 'gpt2'\n",
    "model=AutoModelForCausalLM.from_pretrained(model_Id)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=100)\n",
    "hf=HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is machine learning and what do you want it to like?\\n\\nMachine learning is extremely difficult to build. For people with strong machine learning skills, it may seem like a challenge, but there are tons of great tools on the market today that can help. Here we will provide a guide for developers and experts to help you with machine learning.\\n\\nWhat is machine learning for young students?\\n\\nIn computer science we believe in the most advanced tools in the field, so we think that computer science students should'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "hf.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use HuggingfacePipelines With Gpu\n",
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,  # replace with device_map=\"auto\" to use the accelerate library.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
